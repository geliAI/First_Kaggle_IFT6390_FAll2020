{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CountVectorizer():\n",
    "    def __init__(self,corpus,max_df=1.0,minLen=2):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        corpus: input corpus\n",
    "        max_df : float, default=1.0\n",
    "            Tokens that have a document frequency strictly higher than the given threshold (corpus-specific\n",
    "            stop words) will be ignored.\n",
    "        minLen: int, default = 2\n",
    "            A token is defined as a combination of minLen or more alphanumeric characters.            \n",
    "        '''\n",
    "\n",
    "        self.corpus = corpus\n",
    "        self.max_df = max_df\n",
    "        self.minLen = 2\n",
    "        self.vocabulary_dic= {}\n",
    "        self.word_token_rule = list(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 _\")\n",
    "        \n",
    "        \n",
    "    def Doc2List(self,document,ifSort=True):\n",
    "\n",
    "        values = self.word_token_rule\n",
    "        \n",
    "        def remover(aString = \"\"):\n",
    "            for item in aString:\n",
    "                if item not in values:\n",
    "                    aString = aString.replace(item, \" \")\n",
    "            return aString\n",
    "\n",
    "        listFromDoc=[element.lower() for element in remover(document).split() if len(element)>=self.minLen]\n",
    "\n",
    "        if ifSort:\n",
    "            listFromDoc.sort()\n",
    "        return listFromDoc\n",
    "    \n",
    "    \n",
    "\n",
    "    def BuildVocabularyDic(self):\n",
    "        # Build Vocabulary from a corpus, \n",
    "        # Return type is dictionary\n",
    "\n",
    "        initial_list=[]\n",
    "        for document in self.corpus:\n",
    "            string_from_doc=self.Doc2List(document,ifSort=False)\n",
    "            initial_list=initial_list+string_from_doc\n",
    "        initial_list=list(set(initial_list))\n",
    "        initial_list.sort()\n",
    "        \n",
    "        vocabulary_dic_len=0\n",
    "        for element in initial_list:\n",
    "            self.vocabulary_dic[element]=vocabulary_dic_len\n",
    "            vocabulary_dic_len += 1\n",
    "        \n",
    "        return None\n",
    "            \n",
    "\n",
    "    def BuildWordCountDocument(self,document):\n",
    "        n_row=1\n",
    "        n_col=len(self.vocabulary_dic)\n",
    "        CountVectorizer_vector = np.zeros((n_row,n_col)).astype('int64')\n",
    "        string_from_doc=self.Doc2List(document)\n",
    "        for string_element in string_from_doc:\n",
    "            if string_element in self.vocabulary_dic:\n",
    "                CountVectorizer_vector[0,self.vocabulary_dic[string_element]] +=1\n",
    "\n",
    "        return CountVectorizer_vector\n",
    "\n",
    "\n",
    "    def BuildWordCountCorpus(self,*args):\n",
    "        \n",
    "        if len(args) ==0:\n",
    "        \n",
    "            n_row=len(self.corpus)\n",
    "            n_col=len(self.vocabulary_dic)\n",
    "            self.CountVectorizer_array = np.zeros((n_row,n_col)).astype('int')\n",
    "\n",
    "            for (i,document) in enumerate(self.corpus):\n",
    "                self.CountVectorizer_array[i,:] = self.BuildWordCountDocument(document)\n",
    "\n",
    "            if self.max_df < 1.0:\n",
    "                self.__cutoff_with_maxdf()\n",
    "                \n",
    "            return None\n",
    "                \n",
    "        elif len(args) == 1:\n",
    "            corpus = args[0]\n",
    "        \n",
    "            n_row=len(corpus)\n",
    "            n_col=len(self.vocabulary_dic)\n",
    "            CountVectorizer_array = np.zeros((n_row,n_col)).astype('int')\n",
    "\n",
    "            for (i,document) in enumerate(corpus):\n",
    "                CountVectorizer_array[i,:] = self.BuildWordCountDocument(document)\n",
    "            \n",
    "            \n",
    "            return CountVectorizer_array\n",
    "        else:\n",
    "            raise(\"Too many arguments.\")\n",
    "            \n",
    "\n",
    "    def __cutoff_with_maxdf(self):\n",
    "\n",
    "        ifExist = self.CountVectorizer_array > 0\n",
    "        docNumber = self.CountVectorizer_array.shape[0]\n",
    "        \n",
    "        df = np.sum(ifExist,axis=0) / docNumber\n",
    "        inds = np.argwhere(df <= self.max_df)\n",
    "        inds =inds.reshape(len(inds),)\n",
    "        VocDic_cutoff = {key: value for key,value in self.vocabulary_dic.items() if value in inds }\n",
    "\n",
    "        CountVectorizer_array_cutoff = self.CountVectorizer_array[:,inds]\n",
    "        \n",
    "        self.vocabulary_dic = VocDic_cutoff\n",
    "        self.CountVectorizer_array = CountVectorizer_array_cutoff\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return list(self.vocabulary_dic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multinomial:\n",
    "    def __init__(self, alpha = 1.0):\n",
    "        self.alpha = alpha\n",
    "    def train(self, train_data):\n",
    "        self.train_data = train_data\n",
    "        self.n_features_ = train_data.shape[1]\n",
    "        self.feature_count_ = np.sum(train_data,axis=0)\n",
    "        \n",
    "        Nyi = self.feature_count_\n",
    "        Ny = np.sum(Nyi)\n",
    "        \n",
    "        self.log_prob = (np.log(Nyi+self.alpha)-np.log(Ny+self.alpha*self.n_features_))\n",
    "\n",
    "    def loglikelihood(self, test_data):\n",
    "        test_log_prob = test_data.dot(self.log_prob)\n",
    "        return test_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesClassifier:\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.maximum_likelihood_models=[]\n",
    "        self.priors = None\n",
    "        self.n_classes = None\n",
    "        \n",
    "        \n",
    "    def train(self,X_train,Y_train):\n",
    "        \n",
    "        label_list=np.unique(Y_train)\n",
    "        \n",
    "        total_num = len(Y_train)\n",
    "        self.n_classes = len(label_list)\n",
    "\n",
    "        model_ml=[]\n",
    "        priors= np.zeros(self.n_classes )\n",
    "        \n",
    "        for i in label_list:\n",
    "            X_train_i = X_train[Y_train == i,:]\n",
    "            priors[i] = len(X_train_i)/total_num\n",
    "            model_class_i = Multinomial(alpha=self.alpha)\n",
    "            model_class_i.train(X_train_i)\n",
    "            model_ml.append(model_class_i)\n",
    "            \n",
    "        \n",
    "        self.maximum_likelihood_models = model_ml\n",
    "        self.priors = priors\n",
    "        \n",
    "        assert len(self.maximum_likelihood_models) == len(self.priors)\n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "    # Returns a matrix of size number of test ex. times number of classes containing the log\n",
    "    # probabilities of each test example under each model, trained by ML. \n",
    "    def loglikelihood(self, test_data):\n",
    "\n",
    "        log_pred = np.zeros((test_data.shape[0], self.n_classes))\n",
    "\n",
    "        for i in range(self.n_classes):\n",
    "            # Here, we will have to use maximum_likelihood_models[i] and priors to fill in\n",
    "            # each column of log_pred (it's more efficient to do a entire column at a time)\n",
    "            log_pred[:, i] = self.maximum_likelihood_models[i].loglikelihood(test_data)+np.log(self.priors[i])\n",
    "\n",
    "        return log_pred\n",
    "    \n",
    "    def get_pred(self,test_data):\n",
    "        # Calculate the log-probabilities according to our model\n",
    "        logprob = self.loglikelihood(test_data) # \n",
    "\n",
    "        # Predict labels\n",
    "        classes_pred = np.argmax(logprob,axis=1)#\n",
    "        return classes_pred \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decscription\n",
    "The code is developed based on the Naive Bayes method with the Multinomial distribution. Key parameters are `max_df`, the upper limit of term document frequency, and $\\alpha$, the smoothing factor used when calculateing the Multinomial probability. With `max_df` = 1.0 and $\\alpha$ =0.5, the code achieves an accuracy of 79.8% in the final testing phase.\n",
    "\n",
    "### Building count matrix\n",
    "```python\n",
    "WordCountModel = CountVectorizer(corpus,max_df=1.0) ### Create an CountVectorizer object###\n",
    "WordCountModel.BuildVocabularyDic()  ### Build vocabulary dictionary ###\n",
    "WordCountModel.BuildWordCountCorpus() ### Construc count matrix ###\n",
    "X = WordCountModel.CountVectorizer_array ### X is the count matrix\n",
    "```\n",
    "### Train and predict using the classifier\n",
    "```python\n",
    "\n",
    "alpha_value=.5\n",
    "clf = BayesClassifier(alpha_value)\n",
    "clf.train(X_train,Y_train)\n",
    "```\n",
    "\n",
    "### Make predictions\n",
    "```python\n",
    "Y_pred_test = clf.get_pred(X_test)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An  example of building count matrix based on given corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    'This is the first document. d',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "### Create an CountVectorizer object###\n",
    "WordCountModel = CountVectorizer(corpus,max_df=1.0)\n",
    "\n",
    "### Build vocabulary dictionary ###\n",
    "WordCountModel.BuildVocabularyDic()\n",
    "\n",
    "### Construc count matrix ###\n",
    "WordCountModel.BuildWordCountCorpus()\n",
    "\n",
    "\n",
    "### Print the vocablary disctionary and count matrix X###\n",
    "print(WordCountModel.get_feature_names())\n",
    "\n",
    "X = WordCountModel.CountVectorizer_array\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproduce Kaggle competition submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Converting categories into class labels...\n",
      "{0: 'astro-ph', 1: 'astro-ph.CO', 2: 'astro-ph.GA', 3: 'astro-ph.SR', 4: 'cond-mat.mes-hall', 5: 'cond-mat.mtrl-sci', 6: 'cs.LG', 7: 'gr-qc', 8: 'hep-ph', 9: 'hep-th', 10: 'math.AP', 11: 'math.CO', 12: 'physics.optics', 13: 'quant-ph', 14: 'stat.ML'}\n"
     ]
    }
   ],
   "source": [
    "## Read data and pre-processing\n",
    "print('Reading data...')\n",
    "\n",
    "import pandas as pd\n",
    "path_to_train_data = './dataset/train.csv'\n",
    "path_to_test_data = './dataset/test.csv'\n",
    "train_data=pd.read_csv(path_to_train_data)\n",
    "corpus=train_data['Abstract']\n",
    "\n",
    "test_data=pd.read_csv(path_to_test_data)\n",
    "corpus_test=test_data['Abstract']\n",
    "\n",
    "print('Converting categories into class labels...')\n",
    "train_data['classes']=train_data['Category'].astype('category').cat.codes\n",
    "category_dict=dict( enumerate(train_data['Category'].astype('category').cat.categories ) )\n",
    "print(category_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing count matrix for both train and test data...\n"
     ]
    }
   ],
   "source": [
    "print('Constructing count matrix for both train and test data...')\n",
    "\n",
    "WordCountModel = CountVectorizer(corpus,max_df=1.0)\n",
    "WordCountModel.BuildVocabularyDic()\n",
    "WordCountModel.BuildWordCountCorpus()\n",
    "X = WordCountModel.CountVectorizer_array\n",
    "Y = np.array(train_data['classes'])\n",
    "\n",
    "X_test=WordCountModel.BuildWordCountCorpus(corpus_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train the model\n",
    "X_train=X\n",
    "Y_train=Y\n",
    "\n",
    "alpha_optima=0.5\n",
    "classifier_optimal = BayesClassifier(alpha_optima)\n",
    "classifier_optimal.train(X_train,Y_train)\n",
    "\n",
    "## get the predictions\n",
    "\n",
    "Y_pred_test = classifier_optimal.get_pred(X_test)\n",
    "\n",
    "## Convert predicted labels into categories and write it into .csv file\n",
    "\n",
    "Y_pred_test_cate = [category_dict[i] for i in Y_pred_test]\n",
    "submission_data=test_data.copy()\n",
    "submission_data['Category']=Y_pred_test_cate\n",
    "submission_data.drop(columns='Abstract',inplace=True)\n",
    "submission_data.to_csv('Kaggle_submission_GeLi.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
